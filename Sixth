import os
import pickle
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split
from torch.optim.lr_scheduler import CosineAnnealingLR
from PIL import Image
from torchsummary import summary
from torch.optim import RAdam
from torch.optim.lr_scheduler import ReduceLROnPlateau

# 自动选择 CPU 或 GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# CIFAR-10 数据集路径（修改为 HPC 服务器的路径）
cifar10_dir = './data/cifar-10-python/cifar-10-batches-py'
cifar_test_path = './data/cifar_test_nolabel.pkl'

# 加载 CIFAR-10 数据集的 batch
def load_cifar_batch(file):
    with open(file, 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
    return dict

# 加载元数据（标签）
meta_data_dict = load_cifar_batch(os.path.join(cifar10_dir, 'batches.meta'))
label_names = [label.decode('utf-8') for label in meta_data_dict[b'label_names']]

# 加载训练数据
train_data = []
train_labels = []
for i in range(1, 6):
    batch = load_cifar_batch(os.path.join(cifar10_dir, f'data_batch_{i}'))
    train_data.append(batch[b'data'])
    train_labels += batch[b'labels']

# 处理数据格式
train_data = np.vstack(train_data).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)
train_labels = np.array(train_labels)

# 定义训练数据的 transform（带数据增强）
train_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.1, contrast=0.1),
    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),
    transforms.RandomErasing(p=0.1, scale=(0.02, 0.08), value=0)
])

# 定义测试数据的 transform
test_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))
])

# 构建训练集（应用 train_transform）
train_dataset = [(train_transform(img), label) for img, label in zip(train_data, train_labels)]

# 划分训练集和验证集
train_size = int(0.9 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])

# 加载测试集
test_batch = load_cifar_batch(cifar_test_path)
raw_data = test_batch[b'data']
# 假设 raw_data 已经是 (10000, 32, 32, 3) 的 HWC 格式
test_images = raw_data.astype(np.uint8)
# 构建测试集（应用 test_transform）
test_dataset = [test_transform(img) for img in test_images]

# 创建 DataLoader
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)


# 定义 MixUp 数据增强函数
def mixup_data(x, y, alpha=0.2):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(device)
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

# 定义 Residual Block 模块
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,
                               stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.skip = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.skip = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1,
                          stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        self.dropout = nn.Dropout(0.3)

    def forward(self, x):
        identity = x
        if self.skip:
            identity = self.skip(x)
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out += identity
        out = self.relu(out)
        out = self.dropout(out)
        return out

# 定义自定义 ResNet 模型
class CustomResNet(nn.Module):
    def __init__(self, num_classes=10):
        super(CustomResNet, self).__init__()
        self.init_conv = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)
        self.init_bn = nn.BatchNorm2d(32)
        self.relu = nn.ReLU(inplace=True)
        # 堆叠多个残差块，每个阶段加深网络
        self.layer1 = nn.Sequential(
            ResidualBlock(32, 64, stride=1),
            ResidualBlock(64, 64, stride=1)
        )
        self.layer2 = nn.Sequential(
            ResidualBlock(64, 128, stride=2),
            ResidualBlock(128, 128, stride=1)
        )
        self.layer3 = nn.Sequential(
            ResidualBlock(128, 256, stride=2),
            ResidualBlock(256, 256, stride=1)
        )
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(256, num_classes)

    def forward(self, x):
        out = self.relu(self.init_bn(self.init_conv(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.avg_pool(out)
        out = torch.flatten(out, 1)
        out = self.fc(out)
        return out

# 输出模型结构和参数总量
model = CustomResNet().to(device)
summary(model, (3, 32, 32))

# 定义训练函数（使用 MixUp）
def train_model(model, train_loader, val_loader, epochs=150):
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = RAdam(model.parameters(), lr=0.001, weight_decay=1e-4)  # 用 RAdam 替换 AdamW
    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, verbose=True)  # 自适应调整学习率

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            
            # 使用 MixUp 数据增强
            images, targets_a, targets_b, lam = mixup_data(images, labels, alpha=0.2)
            outputs = model(images)
            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)
            
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        # 验证阶段（不使用 MixUp）
        model.eval()
        correct = 0
        total = 0
        val_loss = 0.0  # 记录验证损失
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)  # 计算验证损失
                val_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        val_acc = 100 * correct / total  # 计算验证集准确率
        val_loss /= len(val_loader)  # 计算平均验证损失

        scheduler.step(val_acc)  # 传入验证集准确率，调整学习率

        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, ' +
              f'Validation Accuracy: {val_acc:.2f}%, Validation Loss: {val_loss:.4f}')

       

# 重新实例化模型（确保训练不受之前权重影响）
model = CustomResNet().to(device)
train_model(model, train_loader, val_loader, epochs=150)

# 计算测试集的预测结果
model.eval()
predictions = []
with torch.no_grad():
    for images in test_loader:  # 确保只取 images
        images = images.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        predictions.extend(predicted.cpu().numpy())


submission = pd.DataFrame({'ID': np.arange(len(predictions)), 'Labels': predictions})
submission.to_csv('./submission.csv', index=False)
print("✅ 提交文件已保存：./submission.csv")

import matplotlib.pyplot as plt

# 随机显示 5 张训练集图片
fig, axes = plt.subplots(1, 5, figsize=(12, 4))
for i in range(5):
    img = train_data[i]  # 直接使用 train_data
    axes[i].imshow(img)
    axes[i].axis("off")
plt.show()

# 随机显示 5 张测试集图片
fig, axes = plt.subplots(1, 5, figsize=(12, 4))
for i in range(5):
    img = test_images[i]  # 直接使用 test_images
    axes[i].imshow(img)
    axes[i].axis("off")
plt.show()


