在本地终端执行：
git clone https://github.com/your-username/deep-learning-cifar10.git
cd deep-learning-cifar10

创建 Python 环境
建议使用 venv 或 conda 创建虚拟环境
python -m venv venv  # 创建虚拟环境
source venv/bin/activate  # Mac

创建 requirements.txt：
numpy
pandas
matplotlib
torch
torchvision
tensorflow
scikit-learn
pickle5
jupyter
然后安装
pip install -r requirements.txt

创建项目结构
mkdir data src models notebooks
touch src/dataloader.py src/train.py src/model.py src/utils.py notebooks/exploration.ipynb README.md
最终结构目录
deep-learning-cifar10/
│── data/                  # 存放 CIFAR-10 数据集
│── notebooks/             # Jupyter Notebook 分析
│── src/                   # 代码文件
│   ├── dataloader.py      # 处理数据加载
│   ├── train.py           # 训练脚本
│   ├── model.py           # 定义模型
│   ├── utils.py           # 其他工具函数
│── requirements.txt       # 依赖文件
│── README.md              # 项目说明


处理数据
在 src/dataloader.py 里写入
import pickle
import numpy as np

def unpickle(file):
    """Load CIFAR-10 batch data"""
    with open(file, 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
    return dict

def load_cifar10_data(data_dir):
    """Load all CIFAR-10 training data and split into train & validation"""
    x_train, y_train = [], []
    for i in range(1, 6):
        batch = unpickle(f"{data_dir}/data_batch_{i}")
        x_train.append(batch[b'data'])
        y_train.extend(batch[b'labels'])

    x_train = np.concatenate(x_train, axis=0)
    y_train = np.array(y_train)
    
    # 80% 训练，20% 验证
    split_idx = int(len(x_train) * 0.8)
    return (x_train[:split_idx], y_train[:split_idx]), (x_train[split_idx:], y_train[split_idx:])

# 测试
if __name__ == "__main__":
    train, val = load_cifar10_data("../data")
    print("Train shape:", train[0].shape, "Val shape:", val[0].shape)

训练模型
在 src/train.py 中写入：
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import DataLoader, TensorDataset

# 定义简单的 CNN 模型
class CIFAR10Model(nn.Module):
    def __init__(self):
        super(CIFAR10Model, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(32 * 16 * 16, 10)

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        return x

# 训练函数
def train(model, train_loader, val_loader, epochs=10):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(epochs):
        model.train()
        for images, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
        print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# 训练
if __name__ == "__main__":
    train_dataset = datasets.CIFAR10(root='../data', train=True, transform=transforms.ToTensor(), download=True)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    
    model = CIFAR10Model()
    train(model, train_loader, None)
