{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b11fc687-4eaa-4af1-a9dc-d125481c4cc1",
   "metadata": {},
   "source": [
    "# Model Optimization for CIFAR-10 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e66d6-b2b5-4937-aef0-aa8fb46cbbb2",
   "metadata": {},
   "source": [
    "This notebook presents the main process of selecting and optimizing the final model for CIFAR-10 classification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ce0141-9e99-4ee3-a15b-dde4834d07f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from torchsummary import summary\n",
    "from torch.optim import RAdam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba3ec3-0a0e-4f85-9f35-6a2159772e87",
   "metadata": {},
   "source": [
    "## Baseline Model (Version 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a60e68-cb3e-42f4-81b3-af00ef485e9b",
   "metadata": {},
   "source": [
    "We built the first version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c259ff-6bfe-425f-ac11-eb5092ad7ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "            Conv2d-4          [-1, 128, 32, 32]           8,192\n",
      "       BatchNorm2d-5          [-1, 128, 32, 32]             256\n",
      "            Conv2d-6          [-1, 128, 32, 32]          73,728\n",
      "       BatchNorm2d-7          [-1, 128, 32, 32]             256\n",
      "              ReLU-8          [-1, 128, 32, 32]               0\n",
      "            Conv2d-9          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-10          [-1, 128, 32, 32]             256\n",
      "             ReLU-11          [-1, 128, 32, 32]               0\n",
      "          Dropout-12          [-1, 128, 32, 32]               0\n",
      "    ResidualBlock-13          [-1, 128, 32, 32]               0\n",
      "           Conv2d-14          [-1, 256, 16, 16]          32,768\n",
      "      BatchNorm2d-15          [-1, 256, 16, 16]             512\n",
      "           Conv2d-16          [-1, 256, 16, 16]         294,912\n",
      "      BatchNorm2d-17          [-1, 256, 16, 16]             512\n",
      "             ReLU-18          [-1, 256, 16, 16]               0\n",
      "           Conv2d-19          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-20          [-1, 256, 16, 16]             512\n",
      "             ReLU-21          [-1, 256, 16, 16]               0\n",
      "          Dropout-22          [-1, 256, 16, 16]               0\n",
      "    ResidualBlock-23          [-1, 256, 16, 16]               0\n",
      "           Conv2d-24            [-1, 512, 8, 8]         131,072\n",
      "      BatchNorm2d-25            [-1, 512, 8, 8]           1,024\n",
      "           Conv2d-26            [-1, 512, 8, 8]       1,179,648\n",
      "      BatchNorm2d-27            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-28            [-1, 512, 8, 8]               0\n",
      "           Conv2d-29            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-30            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-31            [-1, 512, 8, 8]               0\n",
      "          Dropout-32            [-1, 512, 8, 8]               0\n",
      "    ResidualBlock-33            [-1, 512, 8, 8]               0\n",
      "AdaptiveAvgPool2d-34            [-1, 512, 1, 1]               0\n",
      "           Linear-35                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 4,829,258\n",
      "Trainable params: 4,829,258\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 19.00\n",
      "Params size (MB): 18.42\n",
      "Estimated Total Size (MB): 37.44\n",
      "----------------------------------------------------------------\n",
      "Epoch 1, Loss: 1.7712, Validation Accuracy: 41.84%\n",
      "Epoch 2, Loss: 1.4700, Validation Accuracy: 55.28%\n",
      "Epoch 3, Loss: 1.3432, Validation Accuracy: 60.56%\n",
      "Epoch 4, Loss: 1.2583, Validation Accuracy: 66.64%\n",
      "Epoch 5, Loss: 1.1870, Validation Accuracy: 66.70%\n",
      "Epoch 6, Loss: 1.1252, Validation Accuracy: 68.56%\n",
      "Epoch 7, Loss: 1.0626, Validation Accuracy: 72.14%\n",
      "Epoch 8, Loss: 1.0087, Validation Accuracy: 76.02%\n",
      "Epoch 9, Loss: 0.9580, Validation Accuracy: 75.22%\n",
      "Epoch 10, Loss: 0.9064, Validation Accuracy: 76.36%\n",
      "Epoch 11, Loss: 0.8587, Validation Accuracy: 76.88%\n",
      "Epoch 12, Loss: 0.8112, Validation Accuracy: 78.14%\n",
      "Epoch 13, Loss: 0.7699, Validation Accuracy: 78.64%\n",
      "Epoch 14, Loss: 0.7305, Validation Accuracy: 79.76%\n",
      "Epoch 15, Loss: 0.6924, Validation Accuracy: 79.46%\n",
      "Epoch 16, Loss: 0.6634, Validation Accuracy: 80.48%\n",
      "Epoch 17, Loss: 0.6411, Validation Accuracy: 80.20%\n",
      "Epoch 18, Loss: 0.6230, Validation Accuracy: 81.28%\n",
      "Epoch 19, Loss: 0.6112, Validation Accuracy: 80.88%\n",
      "Epoch 20, Loss: 0.5982, Validation Accuracy: 81.44%\n",
      "Epoch 21, Loss: 0.5907, Validation Accuracy: 81.14%\n",
      "Epoch 22, Loss: 0.5824, Validation Accuracy: 81.44%\n",
      "Epoch 23, Loss: 0.5771, Validation Accuracy: 81.42%\n",
      "Epoch 24, Loss: 0.5714, Validation Accuracy: 81.60%\n",
      "Epoch 25, Loss: 0.5664, Validation Accuracy: 80.98%\n",
      "Epoch 26, Loss: 0.5625, Validation Accuracy: 81.44%\n",
      "Epoch 27, Loss: 0.5585, Validation Accuracy: 81.54%\n",
      "Epoch 28, Loss: 0.5561, Validation Accuracy: 80.86%\n",
      "Epoch 29, Loss: 0.5520, Validation Accuracy: 81.90%\n",
      "Epoch 30, Loss: 0.5490, Validation Accuracy: 82.02%\n",
      "Epoch 31, Loss: 0.5471, Validation Accuracy: 82.26%\n",
      "Epoch 32, Loss: 0.5453, Validation Accuracy: 82.32%\n",
      "Epoch 33, Loss: 0.5433, Validation Accuracy: 82.58%\n",
      "Epoch 34, Loss: 0.5411, Validation Accuracy: 82.26%\n",
      "Epoch 35, Loss: 0.5401, Validation Accuracy: 82.64%\n",
      "Epoch 36, Loss: 0.5378, Validation Accuracy: 82.82%\n",
      "Epoch 37, Loss: 0.5366, Validation Accuracy: 82.76%\n",
      "Epoch 38, Loss: 0.5355, Validation Accuracy: 82.72%\n",
      "Epoch 39, Loss: 0.5340, Validation Accuracy: 83.22%\n",
      "Epoch 40, Loss: 0.5330, Validation Accuracy: 83.20%\n",
      "Epoch 41, Loss: 0.5320, Validation Accuracy: 83.06%\n",
      "Epoch 42, Loss: 0.5314, Validation Accuracy: 82.94%\n",
      "Epoch 43, Loss: 0.5306, Validation Accuracy: 83.08%\n",
      "Epoch 44, Loss: 0.5299, Validation Accuracy: 83.14%\n",
      "Epoch 45, Loss: 0.5295, Validation Accuracy: 83.14%\n",
      "Epoch 46, Loss: 0.5292, Validation Accuracy: 83.20%\n",
      "Epoch 47, Loss: 0.5290, Validation Accuracy: 83.02%\n",
      "Epoch 48, Loss: 0.5289, Validation Accuracy: 83.14%\n",
      "Epoch 49, Loss: 0.5287, Validation Accuracy: 83.06%\n",
      "Epoch 50, Loss: 0.5284, Validation Accuracy: 83.18%\n",
      "Final Validation Set Accuracy: 83.18%\n"
     ]
    }
   ],
   "source": [
    "# Automatically select CPU or GPU for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Define CIFAR-10 dataset paths\n",
    "cifar10_dir = './data/cifar-10-python/cifar-10-batches-py'\n",
    "cifar_test_path = './data/cifar_test_nolabel.pkl'\n",
    "\n",
    "# Function to load CIFAR-10 batch files\n",
    "def load_cifar_batch(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Load metadata\n",
    "meta_data_dict = load_cifar_batch(os.path.join(cifar10_dir, 'batches.meta'))\n",
    "label_names = [label.decode('utf-8') for label in meta_data_dict[b'label_names']]\n",
    "\n",
    "# Load training data\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for i in range(1, 6):\n",
    "    batch = load_cifar_batch(os.path.join(cifar10_dir, f'data_batch_{i}'))\n",
    "    train_data.append(batch[b'data'])\n",
    "    train_labels += batch[b'labels']\n",
    "\n",
    "# Convert data to the correct format (HWC)\n",
    "train_data = np.vstack(train_data).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Define data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),  # Apply random affine transformations\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "    transforms.RandomErasing(p=0.1, scale=(0.02, 0.08), value=0)  # Reduce random erasing strength\n",
    "])\n",
    "\n",
    "# Convert to TensorDataset and apply transformation\n",
    "train_dataset = [(transform(img), label) for img, label in zip(train_data, train_labels)]\n",
    "\n",
    "# Split dataset into training (90%) and validation (10%)\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Load test dataset\n",
    "test_batch = load_cifar_batch(cifar_test_path)\n",
    "test_images = test_batch[b'data'].astype(np.float32) / 255.0\n",
    "test_images = test_images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)  \n",
    "\n",
    "# Convert test dataset to Tensor format\n",
    "test_dataset = [(transform(img)) for img in test_images]\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model, train_loader, val_loader, epochs=50):\n",
    "    # CrossEntropyLoss with label smoothing to improve generalization\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    # Use AdamW optimizer with weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.003, weight_decay=5e-5)\n",
    "    # Cosine annealing learning rate scheduler\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train() \n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device) \n",
    "            optimizer.zero_grad() \n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval() \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad(): \n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1) \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Validation Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "# Define Residual Block module\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Skip connection for residual learning\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.skip:\n",
    "            identity = self.skip(x)\n",
    "        # Forward pass through the two convolutional layers\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity  # Add residual connection\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "# Define Custom ResNet model\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.init_conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.init_bn = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Define multiple residual layers\n",
    "        self.layer1 = ResidualBlock(64, 128, stride=1)\n",
    "        self.layer2 = ResidualBlock(128, 256, stride=2)\n",
    "        self.layer3 = ResidualBlock(256, 512, stride=2)\n",
    "\n",
    "        # Global average pooling and fully connected layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.init_conv(x)\n",
    "        out = self.init_bn(out)\n",
    "        out = self.relu(out)\n",
    "        # Forward pass through each residual block stage\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # Global average pooling\n",
    "        out = self.avg_pool(out)\n",
    "        out = torch.flatten(out, 1) # Flatten for fully connected layer\n",
    "        out = self.fc(out) \n",
    "        return out\n",
    "\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, dataloader, dataset_name=\"Dataset\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # Forward pass\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    # Compute final accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Final {dataset_name} Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Train the model\n",
    "model = CustomResNet().to(device)\n",
    "# Print parameters\n",
    "summary(model, (3, 32, 32))\n",
    "# Train the model for 50 epochs\n",
    "train_model(model, train_loader, val_loader, epochs=50)\n",
    "final_val_accuracy = evaluate_model(model, val_loader, \"Validation Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0093f6fb-4e14-4d6b-a670-9e3d59e52c59",
   "metadata": {},
   "source": [
    "## Model with MixUp (Version 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379ec73-9936-4bc2-add1-7db775195713",
   "metadata": {},
   "source": [
    "In this version, MixUp augmentation is introduced to prevent overfitting and improve generalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5173c2c5-bc6f-4945-a855-608bf6cec134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "            Conv2d-4          [-1, 128, 32, 32]           8,192\n",
      "       BatchNorm2d-5          [-1, 128, 32, 32]             256\n",
      "            Conv2d-6          [-1, 128, 32, 32]          73,728\n",
      "       BatchNorm2d-7          [-1, 128, 32, 32]             256\n",
      "              ReLU-8          [-1, 128, 32, 32]               0\n",
      "            Conv2d-9          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-10          [-1, 128, 32, 32]             256\n",
      "             ReLU-11          [-1, 128, 32, 32]               0\n",
      "          Dropout-12          [-1, 128, 32, 32]               0\n",
      "    ResidualBlock-13          [-1, 128, 32, 32]               0\n",
      "           Conv2d-14          [-1, 256, 16, 16]          32,768\n",
      "      BatchNorm2d-15          [-1, 256, 16, 16]             512\n",
      "           Conv2d-16          [-1, 256, 16, 16]         294,912\n",
      "      BatchNorm2d-17          [-1, 256, 16, 16]             512\n",
      "             ReLU-18          [-1, 256, 16, 16]               0\n",
      "           Conv2d-19          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-20          [-1, 256, 16, 16]             512\n",
      "             ReLU-21          [-1, 256, 16, 16]               0\n",
      "          Dropout-22          [-1, 256, 16, 16]               0\n",
      "    ResidualBlock-23          [-1, 256, 16, 16]               0\n",
      "           Conv2d-24            [-1, 512, 8, 8]         131,072\n",
      "      BatchNorm2d-25            [-1, 512, 8, 8]           1,024\n",
      "           Conv2d-26            [-1, 512, 8, 8]       1,179,648\n",
      "      BatchNorm2d-27            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-28            [-1, 512, 8, 8]               0\n",
      "           Conv2d-29            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-30            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-31            [-1, 512, 8, 8]               0\n",
      "          Dropout-32            [-1, 512, 8, 8]               0\n",
      "    ResidualBlock-33            [-1, 512, 8, 8]               0\n",
      "AdaptiveAvgPool2d-34            [-1, 512, 1, 1]               0\n",
      "           Linear-35                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 4,829,258\n",
      "Trainable params: 4,829,258\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 19.00\n",
      "Params size (MB): 18.42\n",
      "Estimated Total Size (MB): 37.44\n",
      "----------------------------------------------------------------\n",
      "Epoch 1, Loss: 1.9255, Validation Accuracy: 44.92%\n",
      "Epoch 2, Loss: 1.6783, Validation Accuracy: 54.52%\n",
      "Epoch 3, Loss: 1.5563, Validation Accuracy: 61.96%\n",
      "Epoch 4, Loss: 1.4875, Validation Accuracy: 60.16%\n",
      "Epoch 5, Loss: 1.4392, Validation Accuracy: 70.06%\n",
      "Epoch 6, Loss: 1.3692, Validation Accuracy: 71.24%\n",
      "Epoch 7, Loss: 1.3501, Validation Accuracy: 73.48%\n",
      "Epoch 8, Loss: 1.2692, Validation Accuracy: 75.46%\n",
      "Epoch 9, Loss: 1.2179, Validation Accuracy: 75.10%\n",
      "Epoch 10, Loss: 1.2045, Validation Accuracy: 76.20%\n",
      "Epoch 11, Loss: 1.1857, Validation Accuracy: 78.54%\n",
      "Epoch 12, Loss: 1.1383, Validation Accuracy: 76.90%\n",
      "Epoch 13, Loss: 1.1123, Validation Accuracy: 78.90%\n",
      "Epoch 14, Loss: 1.1034, Validation Accuracy: 78.20%\n",
      "Epoch 15, Loss: 1.0893, Validation Accuracy: 81.92%\n",
      "Epoch 16, Loss: 1.0382, Validation Accuracy: 80.84%\n",
      "Epoch 17, Loss: 0.9935, Validation Accuracy: 80.88%\n",
      "Epoch 18, Loss: 0.9948, Validation Accuracy: 81.70%\n",
      "Epoch 19, Loss: 0.9867, Validation Accuracy: 82.64%\n",
      "Epoch 20, Loss: 0.9399, Validation Accuracy: 81.90%\n",
      "Epoch 21, Loss: 0.9440, Validation Accuracy: 81.94%\n",
      "Epoch 22, Loss: 0.9215, Validation Accuracy: 82.86%\n",
      "Epoch 23, Loss: 0.8970, Validation Accuracy: 83.20%\n",
      "Epoch 24, Loss: 0.9101, Validation Accuracy: 82.54%\n",
      "Epoch 25, Loss: 0.9427, Validation Accuracy: 83.06%\n",
      "Epoch 26, Loss: 0.9109, Validation Accuracy: 82.72%\n",
      "Epoch 27, Loss: 0.8791, Validation Accuracy: 82.74%\n",
      "Epoch 28, Loss: 0.8955, Validation Accuracy: 83.72%\n",
      "Epoch 29, Loss: 0.9334, Validation Accuracy: 82.64%\n",
      "Epoch 30, Loss: 0.8742, Validation Accuracy: 83.88%\n",
      "Epoch 31, Loss: 0.8664, Validation Accuracy: 83.44%\n",
      "Epoch 32, Loss: 0.8810, Validation Accuracy: 83.80%\n",
      "Epoch 33, Loss: 0.8912, Validation Accuracy: 83.84%\n",
      "Epoch 34, Loss: 0.8531, Validation Accuracy: 83.54%\n",
      "Epoch 35, Loss: 0.8573, Validation Accuracy: 84.20%\n",
      "Epoch 36, Loss: 0.8630, Validation Accuracy: 84.64%\n",
      "Epoch 37, Loss: 0.8373, Validation Accuracy: 84.62%\n",
      "Epoch 38, Loss: 0.8610, Validation Accuracy: 84.50%\n",
      "Epoch 39, Loss: 0.8638, Validation Accuracy: 84.22%\n",
      "Epoch 40, Loss: 0.8588, Validation Accuracy: 84.70%\n",
      "Epoch 41, Loss: 0.8548, Validation Accuracy: 84.94%\n",
      "Epoch 42, Loss: 0.8342, Validation Accuracy: 84.96%\n",
      "Epoch 43, Loss: 0.8181, Validation Accuracy: 84.88%\n",
      "Epoch 44, Loss: 0.8316, Validation Accuracy: 84.84%\n",
      "Epoch 45, Loss: 0.8235, Validation Accuracy: 84.82%\n",
      "Epoch 46, Loss: 0.8243, Validation Accuracy: 84.96%\n",
      "Epoch 47, Loss: 0.8340, Validation Accuracy: 84.98%\n",
      "Epoch 48, Loss: 0.8696, Validation Accuracy: 84.58%\n",
      "Epoch 49, Loss: 0.8368, Validation Accuracy: 85.04%\n",
      "Epoch 50, Loss: 0.8539, Validation Accuracy: 84.98%\n",
      "Final Validation Set Accuracy: 84.98%\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "# This part of the code design is no different from the baseline model\n",
    "#######\n",
    "\n",
    "# Define MixUp data augmentation function\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "    \n",
    "# Define the training function\n",
    "def train_model(model, train_loader, val_loader, epochs=50):\n",
    "    # CrossEntropyLoss with label smoothing to improve generalization\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    # Use AdamW optimizer with weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.003, weight_decay=5e-5)\n",
    "    # Cosine annealing learning rate scheduler\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train() \n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device) \n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            # Apply MixUp data augmentation\n",
    "            images, targets_a, targets_b, lam = mixup_data(images, labels, alpha=0.2)\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
    "            \n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Validation Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "# Define Residual Block module\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Skip connection for residual learning\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.skip:\n",
    "            identity = self.skip(x)\n",
    "        # Forward pass through the two convolutional layers\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity  # Add residual connection\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define Custom ResNet model\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.init_conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.init_bn = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Define multiple residual layers\n",
    "        self.layer1 = ResidualBlock(64, 128, stride=1)\n",
    "        self.layer2 = ResidualBlock(128, 256, stride=2)\n",
    "        self.layer3 = ResidualBlock(256, 512, stride=2)\n",
    "\n",
    "        # Global average pooling and fully connected layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.init_conv(x)\n",
    "        out = self.init_bn(out)\n",
    "        out = self.relu(out)\n",
    "        # Forward pass through each residual block stage\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # Global average pooling\n",
    "        out = self.avg_pool(out)\n",
    "        out = torch.flatten(out, 1) # Flatten for fully connected layer\n",
    "        out = self.fc(out) \n",
    "        return out\n",
    "\n",
    "        \n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, dataloader, dataset_name=\"Dataset\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # Forward pass\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    # Compute final accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Final {dataset_name} Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Train the model\n",
    "model = CustomResNet().to(device)\n",
    "# Print parameters\n",
    "summary(model, (3, 32, 32))\n",
    "# Train the model for 50 epochs\n",
    "train_model(model, train_loader, val_loader, epochs=50)\n",
    "final_val_accuracy = evaluate_model(model, val_loader, \"Validation Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44a078c-8627-4802-a8df-7ff2f508a23a",
   "metadata": {},
   "source": [
    "The accuracy of the second version (Model with MixUp) is slightly improved over the first version (Baseline Model), so the next step is to further optimize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c5139e-dab5-4d67-965a-9ef33363c844",
   "metadata": {},
   "source": [
    "## Refined Model with Adjusted Residual Blocks (Version 3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc52a5-0c6c-4ad6-b7fb-25f518ed759c",
   "metadata": {},
   "source": [
    "This version increases model depth by adding multiple residual blocks per stage and adjusts channel sizes from 64 → 512 to 32 → 256 to reduce parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f4c515-c6bd-44d4-b377-b65db1865efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]             864\n",
      "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
      "              ReLU-3           [-1, 32, 32, 32]               0\n",
      "            Conv2d-4           [-1, 64, 32, 32]           2,048\n",
      "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
      "            Conv2d-6           [-1, 64, 32, 32]          18,432\n",
      "       BatchNorm2d-7           [-1, 64, 32, 32]             128\n",
      "              ReLU-8           [-1, 64, 32, 32]               0\n",
      "            Conv2d-9           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-10           [-1, 64, 32, 32]             128\n",
      "             ReLU-11           [-1, 64, 32, 32]               0\n",
      "          Dropout-12           [-1, 64, 32, 32]               0\n",
      "    ResidualBlock-13           [-1, 64, 32, 32]               0\n",
      "           Conv2d-14           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
      "             ReLU-16           [-1, 64, 32, 32]               0\n",
      "           Conv2d-17           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-18           [-1, 64, 32, 32]             128\n",
      "             ReLU-19           [-1, 64, 32, 32]               0\n",
      "          Dropout-20           [-1, 64, 32, 32]               0\n",
      "    ResidualBlock-21           [-1, 64, 32, 32]               0\n",
      "           Conv2d-22          [-1, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
      "           Conv2d-24          [-1, 128, 16, 16]          73,728\n",
      "      BatchNorm2d-25          [-1, 128, 16, 16]             256\n",
      "             ReLU-26          [-1, 128, 16, 16]               0\n",
      "           Conv2d-27          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-28          [-1, 128, 16, 16]             256\n",
      "             ReLU-29          [-1, 128, 16, 16]               0\n",
      "          Dropout-30          [-1, 128, 16, 16]               0\n",
      "    ResidualBlock-31          [-1, 128, 16, 16]               0\n",
      "           Conv2d-32          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-33          [-1, 128, 16, 16]             256\n",
      "             ReLU-34          [-1, 128, 16, 16]               0\n",
      "           Conv2d-35          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 16, 16]             256\n",
      "             ReLU-37          [-1, 128, 16, 16]               0\n",
      "          Dropout-38          [-1, 128, 16, 16]               0\n",
      "    ResidualBlock-39          [-1, 128, 16, 16]               0\n",
      "           Conv2d-40            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 8, 8]             512\n",
      "           Conv2d-42            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-43            [-1, 256, 8, 8]             512\n",
      "             ReLU-44            [-1, 256, 8, 8]               0\n",
      "           Conv2d-45            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-46            [-1, 256, 8, 8]             512\n",
      "             ReLU-47            [-1, 256, 8, 8]               0\n",
      "          Dropout-48            [-1, 256, 8, 8]               0\n",
      "    ResidualBlock-49            [-1, 256, 8, 8]               0\n",
      "           Conv2d-50            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-51            [-1, 256, 8, 8]             512\n",
      "             ReLU-52            [-1, 256, 8, 8]               0\n",
      "           Conv2d-53            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-54            [-1, 256, 8, 8]             512\n",
      "             ReLU-55            [-1, 256, 8, 8]               0\n",
      "          Dropout-56            [-1, 256, 8, 8]               0\n",
      "    ResidualBlock-57            [-1, 256, 8, 8]               0\n",
      "AdaptiveAvgPool2d-58            [-1, 256, 1, 1]               0\n",
      "           Linear-59                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 2,760,490\n",
      "Trainable params: 2,760,490\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 16.50\n",
      "Params size (MB): 10.53\n",
      "Estimated Total Size (MB): 27.04\n",
      "----------------------------------------------------------------\n",
      "Epoch 1, Loss: 1.9289, Validation Accuracy: 43.10%\n",
      "Epoch 2, Loss: 1.7123, Validation Accuracy: 51.14%\n",
      "Epoch 3, Loss: 1.5761, Validation Accuracy: 59.58%\n",
      "Epoch 4, Loss: 1.5240, Validation Accuracy: 58.80%\n",
      "Epoch 5, Loss: 1.4578, Validation Accuracy: 66.98%\n",
      "Epoch 6, Loss: 1.4108, Validation Accuracy: 70.06%\n",
      "Epoch 7, Loss: 1.3616, Validation Accuracy: 71.38%\n",
      "Epoch 8, Loss: 1.3150, Validation Accuracy: 68.90%\n",
      "Epoch 9, Loss: 1.3026, Validation Accuracy: 71.08%\n",
      "Epoch 10, Loss: 1.2929, Validation Accuracy: 76.72%\n",
      "Epoch 11, Loss: 1.2276, Validation Accuracy: 76.04%\n",
      "Epoch 12, Loss: 1.1933, Validation Accuracy: 74.36%\n",
      "Epoch 13, Loss: 1.2023, Validation Accuracy: 79.40%\n",
      "Epoch 14, Loss: 1.1774, Validation Accuracy: 78.72%\n",
      "Epoch 15, Loss: 1.1323, Validation Accuracy: 81.40%\n",
      "Epoch 16, Loss: 1.1139, Validation Accuracy: 80.78%\n",
      "Epoch 17, Loss: 1.1165, Validation Accuracy: 82.06%\n",
      "Epoch 18, Loss: 1.0756, Validation Accuracy: 82.28%\n",
      "Epoch 19, Loss: 1.0352, Validation Accuracy: 83.06%\n",
      "Epoch 20, Loss: 1.0458, Validation Accuracy: 82.34%\n",
      "Epoch 21, Loss: 1.0539, Validation Accuracy: 83.34%\n",
      "Epoch 22, Loss: 0.9946, Validation Accuracy: 82.60%\n",
      "Epoch 23, Loss: 1.0017, Validation Accuracy: 83.32%\n",
      "Epoch 24, Loss: 1.0030, Validation Accuracy: 84.08%\n",
      "Epoch 25, Loss: 0.9962, Validation Accuracy: 83.98%\n",
      "Epoch 26, Loss: 0.9996, Validation Accuracy: 84.90%\n",
      "Epoch 27, Loss: 0.9614, Validation Accuracy: 84.66%\n",
      "Epoch 28, Loss: 0.9679, Validation Accuracy: 84.04%\n",
      "Epoch 29, Loss: 0.9060, Validation Accuracy: 84.58%\n",
      "Epoch 30, Loss: 0.9286, Validation Accuracy: 84.82%\n",
      "Epoch 31, Loss: 0.9793, Validation Accuracy: 85.40%\n",
      "Epoch 32, Loss: 0.9091, Validation Accuracy: 84.64%\n",
      "Epoch 33, Loss: 0.8836, Validation Accuracy: 84.46%\n",
      "Epoch 34, Loss: 0.9254, Validation Accuracy: 85.38%\n",
      "Epoch 35, Loss: 0.9529, Validation Accuracy: 85.28%\n",
      "Epoch 36, Loss: 0.8602, Validation Accuracy: 85.22%\n",
      "Epoch 37, Loss: 0.8879, Validation Accuracy: 85.36%\n",
      "Epoch 38, Loss: 0.8792, Validation Accuracy: 86.02%\n",
      "Epoch 39, Loss: 0.8691, Validation Accuracy: 85.70%\n",
      "Epoch 40, Loss: 0.8924, Validation Accuracy: 85.54%\n",
      "Epoch 41, Loss: 0.8891, Validation Accuracy: 85.60%\n",
      "Epoch 42, Loss: 0.9455, Validation Accuracy: 86.06%\n",
      "Epoch 43, Loss: 0.8947, Validation Accuracy: 85.76%\n",
      "Epoch 44, Loss: 0.8767, Validation Accuracy: 85.84%\n",
      "Epoch 45, Loss: 0.8676, Validation Accuracy: 86.28%\n",
      "Epoch 46, Loss: 0.9048, Validation Accuracy: 86.18%\n",
      "Epoch 47, Loss: 0.8936, Validation Accuracy: 85.84%\n",
      "Epoch 48, Loss: 0.8627, Validation Accuracy: 86.24%\n",
      "Epoch 49, Loss: 0.8454, Validation Accuracy: 86.12%\n",
      "Epoch 50, Loss: 0.8815, Validation Accuracy: 86.20%\n",
      "Final Validation Set Accuracy: 86.20%\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "# This part of the code design is no different from the Model with MixUp version\n",
    "#######\n",
    "\n",
    "\n",
    "# Define Residual Block module\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # First convolutional layer in the residual block\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # Second convolutional layer in the residual block\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Skip connection for residual learning\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.skip:\n",
    "            identity = self.skip(x) \n",
    "        # Forward pass through the two convolutional layers\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        # Add the skip connection (residual connection)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        # Initial convolutional layer\n",
    "        self.init_conv = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.init_bn = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # Modified residual block structure with multiple residual blocks per stage\n",
    "        self.layer1 = nn.Sequential(ResidualBlock(32, 64, stride=1), ResidualBlock(64, 64, stride=1))\n",
    "        self.layer2 = nn.Sequential(ResidualBlock(64, 128, stride=2), ResidualBlock(128, 128, stride=1))\n",
    "        self.layer3 = nn.Sequential(ResidualBlock(128, 256, stride=2), ResidualBlock(256, 256, stride=1))\n",
    "        # Global average pooling and fully connected layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.init_conv(x)\n",
    "        out = self.init_bn(out)\n",
    "        out = self.relu(out)\n",
    "        # Forward pass through each residual block stage\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # Global average pooling\n",
    "        out = self.avg_pool(out)\n",
    "        out = torch.flatten(out, 1) # Flatten for fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, dataloader, dataset_name=\"Dataset\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # Forward pass\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    # Compute final accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Final {dataset_name} Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Train the model\n",
    "model = CustomResNet().to(device)\n",
    "# Print parameters\n",
    "summary(model, (3, 32, 32))\n",
    "# Train the model for 50 epochs\n",
    "train_model(model, train_loader, val_loader, epochs=50)\n",
    "final_val_accuracy = evaluate_model(model, val_loader, \"Validation Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a738f-ba73-4150-a955-6463cb9174be",
   "metadata": {},
   "source": [
    "After improving the model structure, the validation accuracy increased, demonstrating the effectiveness of architectural refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f471d8-9291-40cd-81f6-131bc66ac5d0",
   "metadata": {},
   "source": [
    "## Optimized Model (Final Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539bc262-69f2-4ffd-84e9-54a4ac0dc008",
   "metadata": {},
   "source": [
    "In the final version of the model, based on the current model, we have carried out the following optimization:\n",
    "1. Data Augmentation Strategy Updated: Applied augmentation only to training data, keeping test data unchanged to improve generalization.\n",
    "2. Optimizer Change: Switched from AdamW to RAdam, with learning rate reduced from 0.003 → 0.001 and weight decay increased from 5e-5 → 1e-4 for better convergence.\n",
    "3. Learning Rate Scheduler Adjustment: Replaced CosineAnnealingLR with ReduceLROnPlateau to dynamically adjust learning rate based on validation performance.\n",
    "4. Enhanced Data Augmentation: Switched from manually defined augmentations (RandomCrop, ColorJitter, RandomAffine) to RandAugment, allowing automated augmentation selection.\n",
    "5. Increased Training Epochs: Raised from 50 → 150 for better learning stability and improved accuracy.\\\n",
    "\\\n",
    "The process of final model is implemented in 'main.ipynb', and the verification accuracy exceeded **90%**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
